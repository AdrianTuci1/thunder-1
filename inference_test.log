ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
âš¡ Thunder PrefixLM: Initializing Inference Engine...
âš¡ Thunder: Loading model unsloth/Llama-3.2-3B-Instruct with 2048 context...
==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.57.6.
   \\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.526 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.35. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
âš¡ Thunder PrefixLM: Adapting architecture for Diffusion (x0-parametrization)...
âš¡ Thunder PrefixLM: Activating Bidirectional Attention...
âš¡ Thunder PrefixLM: Adapting architecture for Diffusion (x0-parametrization)...
âš¡ Thunder PrefixLM: Activating Bidirectional Attention...
âš¡ Loading trained weights from /workspace/thunder/thunder_prefixlm_llama/checkpoint-1000...
âš¡ Thunder PrefixLM: Generating (Steps: 250, CFG: 1.0, Standardized: True)...

==================================================
PROMPT:
### User:
What is the sum of 5 and 7?

### Assistant:


GENERATED:
,,,,      .                                   
==================================================

âš¡ Thunder PrefixLM: Initializing Inference Engine...
âš¡ Thunder: Loading model unsloth/Llama-3.2-3B-Instruct with 2048 context...
==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.57.6.
   \\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.526 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.35. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
âš¡ Thunder PrefixLM: Adapting architecture for Diffusion (x0-parametrization)...
âš¡ Thunder PrefixLM: Activating Bidirectional Attention...
âš¡ Thunder PrefixLM: Adapting architecture for Diffusion (x0-parametrization)...
âš¡ Thunder PrefixLM: Activating Bidirectional Attention...
âš¡ Loading trained weights from /workspace/thunder/thunder_prefixlm_llama/checkpoint-1000...
âš¡ Thunder PrefixLM: Generating (Steps: 250, CFG: 1.0, Standardized: True)...

==================================================
PROMPT:
### User:
Write a python hello world function.

### Assistant:


GENERATED:
,,, . [.                                   
==================================================

âš¡ Thunder PrefixLM: Initializing Inference Engine...
âš¡ Thunder: Loading model unsloth/Llama-3.2-3B-Instruct with 2048 context...
==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.57.6.
   \\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.526 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.35. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
âš¡ Thunder PrefixLM: Adapting architecture for Diffusion (x0-parametrization)...
âš¡ Thunder PrefixLM: Activating Bidirectional Attention...
âš¡ Thunder PrefixLM: Adapting architecture for Diffusion (x0-parametrization)...
âš¡ Thunder PrefixLM: Activating Bidirectional Attention...
âš¡ Loading trained weights from /workspace/thunder/thunder_prefixlm_llama/checkpoint-1000...
âš¡ Thunder PrefixLM: Generating (Steps: 250, CFG: 1.0, Standardized: True)...

==================================================
PROMPT:
### User:
The quick brown fox jumps over the lazy dog.

### Assistant:


GENERATED:
,,  [  
,,-..                                   
==================================================

